# [My trivia Twine game can be found here](http://sarahollett.github.io/DHTriviaTwine/)

## Pedagogy
For my final project in Digital History I chose to create an interactive trivia game using Twine because everyone loves trivia! I created 10 questions that covered a wide range of topics from our class, from small tips and tricks picked up along the way, to more complicated analysis. The game includes a score variable that adjusts as you win or lose points throughout the game. 

When I first started thinking about what I wanted to do for my final project in HIST 5702, I wanted to pick a tool and use it from start to finish. I was originally thinking I wanted to go in depth with topic modeling because I was interested in its potential for my own research. As I thought more about the project and its goals, I realized that using a tool that I was interested in did not really fit the pedagogical structure of the assignment in function or structure. The major outcome of the final project from our course syllabus was to demonstrate "an awareness of the ways digital history can be used to bridge professional practice and the public." Critically reflecting on what I wanted to do, I questioned whether making a project out of something I personally was interested in would really fulfill this outcome. It seemed like a selfish desire that would not reach the widest range of 'public' interests.

For me to focus on my own interests would present less of an engagement with the areas that others may be struggling with in Digital History. All students engage differently with Digital History and I think it's best that we attempt to provide various situations for them to learn. The questions that I created for my trivia game varied from mostly visual clues to carefully explained text that outlined a particular scenario. I felt that it was also important to create examples that we had not seen before. Just as answers on a test usually differ from exact questions done in class, using similar yet different examples was a good way to re-enforce some of the main points from our class. The idea was to challenge players, but not trick them. In the end, my hope is that it forces us to really remember and take some final lessons forward from the course. That being said, I did create some questions that reflect on my personal mistakes, not selfishly, but to illuminate problems that others may have/will come across.

I liked the idea of doing a trivia game because the game works incrementally and with a notion of progress. In Emily Bembeneck's piece [Changing the Classroom Perspective: Incremental Progression](http://www.playthepast.org/?p=2058), she argues that learning should be like a video game. No one starts with 50 points in a game and works their way down and no one should start with an A in a course and work their way down to a lower grade. The trivia style game that begins with a score of 0 mirrors this philosophy of a positive view of achievement. In the classroom, the strategy behind this philosophy is that when each achievement brings a student to the next level, they want to keep achieving. Trivia, in a less concrete sense, works in the same way. You get a question wrong, and instead of feeling like you're falling far away from the mark, you are instead ready and excited to tackle the next question. Think of pub trivia when you lose a round but someone inevitably says: "it's okay, there's always the next round!"

The quick recognition and recall skills required to be a good trivia player are valuable to a Digital Historian. Knowing exactly all the details of how to complete a task always comes after knowing what it is you should set out to do in the first place. I was particularly interested in some of the elements in class that we had not spend a great deal of time focusing on in class. As I did the tutorials, I often picked up on the warnings at the beginning of a tutorial about why this tool might not be useful to us. The questions I created often use these warnings as their base. I present a piece of data or a scenario and the player must decide if this is the best tool for them to use to achieve the desired result. For me, this was one of my greatest take-away messages of the class: that all of the tools are great and fun, but the best tool is the one that works. 

## Methodology
Other than these pedagogical reasons for choosing a trivia game, creating a trivia game was highly compatible with what I wanted to do, as well as with Twine. One could argue that an interactive trivia is no different than a video game, just testing a more traditional breadth of skills in the forms of direct questions. Trivia also seemed like a good choice because I could shape it to be in a format that did not necessarily need to be played in class time as a group. Creating questions that left time for a large group of players to come together for an answer did not facilitate individual play because there would be less discussion and more of a need for a "best answer" situation. It did not make sense to create a game that had open answers instead of a multiple choice style because then the scoring system would fall apart. That being said, the format is still amenable to group play and discussion. 

Twine is also a particularly good format for a trivia game because it allows for different pathways based on the right and wrong answers you give. This way, the flow of the game is fluid from question to question and it plays like a video game. Twine also made it easy to visualize how the trivia would operate. The ability to move the dialogue boxes in Twine to make an intelligible pattern was very valuable to ensure that I did not miss a step or a hook tied to the next question.

## Tools
The tools provided by Twine proved both useful and frustrating. The variable function worked perfectly to set a score that was constantly changing throughout the game. I was able to set `$score` as a variable on the Start dialogue box. From there, each right or wrong answer had the command `set: $score to $score (+/- points)`. At the start of each new question, I used `(print: $score)` so players would know their status throughout. At first, I was trying to manually re-set the `$score` variable for each dialogue box, but quickly realized that once I hit the successive questions I would never be able to do that because the pathway options for an individual's score could never be predicted. After some reviewing of the Harlowe manual and help from classmates, I was on my way. 

## Limitations
The Harlowe manual was not the easiest document to follow. It used some language that I was not clear on. Or perhaps, I just did not understand the more complicated functions that might not apply to my Twine. Oddly, the manual did not cover how to include an image in the game, which was critical for the screenshots I had created for my questions. I ended up finding the command I needed in the Twine form: `<img src="Filename.png">`. When putting my Twine online, I needed to make sure that I had my images files in my GitHub account so they would appear in HTML online. With various versions and programs for Twine like Sugarcube, there seemed to be so many different commands and instructions floating around on the Internet that it became difficult to navigate the correct version I needed when using Twine 2's Harlowe. 

Another limitation of my project was my lack of knowledge of CSS stylesheet. At first, it seemed that I would be able to use the background macro function that was included in the Harlowe manual `(background: red + white) [Pink]` but it turned out that that command was actually only for making the background of a hook text. I realized pretty quickly that to make my game have a nice style other than a completely plain white background, I needed to add a story sheet written in CSS. I knew that I would not be able to create a CSS style sheet and learn the language in time to produce anything worthy for my final project. However, others have created style sheets for Twine 2 and I decided it was better to use a style that had already been [designed](https://www.glorioustrainwrecks.com/node/5163) because I think that is the point of open access.

## Individual Question Paradata

### Question 1
This question comes from the very beginning of the course when we were first learning how to write in [Markdown](http://programminghistorian.org/lessons/getting-started-with-markdown). While Markdown is a fairly simple writing syntax, it does require some work to get the hang off. Because we usually work with the Chicago Manual of Style, footnotes were of particular importance. During class discussions, we discussed how to create footnotes with Markdown. I created this question because I remember there being some slight confusion over footnotes. The reference style links, as they are called, enable you to make footnotes but you need to just not include the entire citation twice. This is a fairly obvious question, but an important point to remember that we can always re-interpret instructions and use them to suit our own needs. The question is worth only one point because the concept was relatively simple.

### Question 2
Question 2 is also a fairly basic concept, but as in Question 1, an important one.  It tests our ability to troubleshot visually, as we often must do on our command lines. It relies on the screenshot to visualize the problem. 

The question has two parts. One, the command `-ls` is not used on Windows machines command lines. This particular problem, for me, was rooted in the frustration that came from the Programming Historian tutorial on the [command line](http://programminghistorian.org/lessons/intro-to-bash) which offered no Windows alternatives other than GitBash. Although GitBash could be a solution, it did not react well with my Windows machine. So I thought that it was still important for us to have a basic understanding of the differences between the Windows command line and bash command line.

The second element of the question relates to the wildcard and the time-saving possibilities of the command line. This portion of the answer comes more for the textual element of the question, rather than the visual. The description of the task indicates that there are an array of file types, but that you only want the text files. This description is meant to trigger our memory of the wildcard function which allows you to narrow down a large search so you can return only results that end with .txt. The correct answer requires the player to understand what the wildcard function is. The right answer here is worth two points because it has these two elements to it. The partially correct answer that does not incorporate the wilcard function is worth one point while the wrong answer for both parts of the question takes away two points.

### Question 3
This one is a broad question that focuses on topic modeling as a concept. It hearkens back to cautionary tales of topic modeling and a historian's temptation to have it answer all of our research questions. In the Programming Historian tutorial on [Topic Modeling](http://programminghistorian.org/lessons/topic-modeling-and-mallet), we are warned that "the proxy is not in itself the thing we seek to understand" and that it is better thought of as a discourse than a topic. 

This question comes out of my own research. Topic modeling was something that I had identified as something that might be of use for documents I may come across because I was particularly interested in how tourism promotion has/does discuss Nova Scotia. I realized pretty quickly that if I was not careful, I could become over-reliant on topic modeling. This question is a realistic pitfall that all of our future research could fall into. Topic modeling could not answer a question that specific. It could help map the answer, but cannot be used to confirm specific hypotheses. I created the question to be worth two points because it is a reasonably complex question which requires attention to the details in the question.

### Question 4
Once again, this question comes from some of my own mistakes and misconceptions throughout the course. It was an easy mistake to make and one that I am sure we could all learn from. Although we ended up not discussing the third [Programming Historian tutorial](http://programminghistorian.org/lessons/counting-frequencies-from-zotero-items) on Zotero, I thought that my experience attempting it was a revealing one. Mostly, it is a lesson on properly reading a tutorial. The idea was to highlight that the tutorial works only for Zotero *HTML* items. To understand this, the player must interpret the screenshot of the code from the tutorial with sample sources not containing URLs. Because it is an exercise in recognizing the relationships between code and corresponding data, I made the question worth two points.

### Question 5
Question 5 is another question that is designed for visual learners, using Internet Archive OCR text from the 1922 book, [*Place-Names of the Province of Nova Scotia*](https://archive.org/details/placenamesofprov00browuoft). The key to answering the question is understanding that the visual layout of the data has an inherent structure to it. The question draws on the tutorials we did on [Regular Expressions](http://programminghistorian.org/lessons/understanding-regular-expressions) and [OpenRefine](http://programminghistorian.org/lessons/cleaning-data-with-openrefine). The regular expressions tutorial advised that when you have data that is not actually tabulated but still has a repetitive and structured format to it, regular expressions can be used to clean it and put into a .csv file. 

I wanted to include another example of such data to emphasize the idea that the simple tool is often the best choice. OpenRefine is useful, but with data like this we can simply use a word processor with regular expression function. Creating this question showed me how difficult coming up with good sample data can be. I wanted to illustrate a potential situation that could occur over the course of research, but finding data that fit my idea for the question proved quite difficult. This furthers the point that until we have a real research situation with high stakes, sample data can only teach us so much about a tool. Again, this question is worth two points because it requires complex thought.

### Question 6
Questions 6 and 7 both use the same corpus from the Carleton University Archives. Knowing that IA had most years of *The Raven*, this sample data was much easier to find because of this previous knowledge. Because AntConc is a longer process, I decided to break it up into two questions. This question is specifically about how to obtain information. 

As we have seen in numerous tutorials, obtaining data can often be a learning process in itself. In Heather Froehlich's tutorial on [AntConc](http://programminghistorian.org/lessons/corpus-analysis-with-antconc), she writes that if your corpus is small enough, it makes more sense to put your text into a text editor and edit it as you go to get a really good sense of the data. Because the OCRed texts of the yearbooks are not long, this method will work well for the data. As well, there are only 10 files so this method makes the most sense and would be beneficial to research. This question is only worth one point because technically both answers are correct, with one answer that is better.

### Question 7
The next question around this corpus is centered around the collocates results and the questions it allows us to ask of our corpus. To prepare the question, I copied and pasted each year of *The Raven* into text files and put them into AntConc. I have imagined the research topic to be sexism and racism on campus in the 1960s. The results returned no context for sexual discrimination on campus, with most results relating to sex as a practice rather than sexism as a concept. 

I used "leads/questions" to describe the options to make the question more open to answer. The idea I am playing off in this question is that the file we have in a corpus determines the sorts of results we get, a point mentioned in the Programming Historian [AntConc tutorial](http://programminghistorian.org/lessons/corpus-analysis-with-antconc). The second answer is a better research question because it does not make final assumptions about the data, but poses an avenue for further research. To say that racism was not part of campus culture in the 1960s is putting too much faith in the files we have accumulated, which always shape our answers. We could have spent more time cleaning the files or compared it to the corpus of yearbooks from the 1970s. This detailed question is worth two points.

### Question 8
This question came out of a reflection I wrote titled ["Knowing Your Data"](http://sarahollett.github.io/Knowing-Your-Data/). I was struck by the idea in the [Generating an Ordered Data Set from a Text File](http://programminghistorian.org/lessons/generating-an-ordered-data-set-from-an-OCR-text-file) tutorial. Although we did not complete the tutorial in class because it was recommended for advanced users, I thought that the tutorial made some good points. It looked at data that was OCRed, but was not regular from the outset. The tutorial involved creating a Python dictionary that would organize the metadata fields so that regular expressions could eventually be used on the data. The question reflects on this tutorial, but is also answerable with knowledge from the tutorials on regular expressions and OpenRefine. 

That reflection was about how the structure of data affects what we can do with it, but not the type. I knew that [Harry Piers' accession ledgers](https://novascotia.ca/archives/Piers/default.asp) had been digitized by the Nova Scotia Archives and that it represented historical metadata that could potentially tell us a lot about the early collecting of the Nova Scotia Museum. The obvious limitation of this theoretical question is that the records are digitized, but do not yet have an OCRed text. So the question (perhaps wrongly) assumes that the very neatly hand-written records could produce OCR texts.  

The rationale for my answer options was that with the knowledge of OpenRefine's abilities and the limits of using regular expressions, the player would see that creating a Python dictionary is the best option. Although we did not do that tutorial in class, it is a valuable tool to be aware of, at the very least. If it was a necessity for our research, delving into the process would be well worth our efforts. As for the two other options, OpenRefine will not work because the data is not structured as a .csv or .tsv file. OpenRefine is more useful for the digitized metadata that we can get from institutions like Powerhouse Museum. Referring back to the tutorial on regular expressions, regular expressions work best if the data is repetitive and clearly structured. The screenshot I have included shows a type of metadata that has an unconventional structure that is not as regular as the 1922 Nova Scotia Place Names data example from Question 5 so regular expressions is not the best option. The type of metadata does not make a difference, but its structure does determine what you should do first. 

### Question 9
This question reinforces the points made in class about data visualization and the effectiveness of Palladio. It focuses on the limitations of Palladio, as outlined in [Miriam Posner's](http://miriamposner.com/blog/getting-started-with-palladio/) tutorial on Pallaido. Palladio is good for visualizing data that has locations associated with it, so the Nova Scotia place names data could be useful. The question is intended to test your knowledge of Palladio's presentation functions. It is not a tool for presenting visualized data in a public presentation, but more for seeing connections during your research process. It also hinges on the idea that Palladio is not yet a fully developed tool, so when you need a quick solution, it could cause you problems.

## Afterthoughts
Overall, I think the project was a success for me. Using the trivia format gave me the freedom to explore different aspects of the class and reaffirm my own knowledge at the same time. The game is definitely aimed at people with a working knowledge of Digital History. Its audience is for people like us who have been exposed to some digital tools, but have not had the opportunity to use them in anyway in their own research. This stage in learning Digital History seems to me to be an awkward one. We have knowledge of the field and things we *can* do, but we have no concrete research-based to do so. So this Twine game is aimed at these people who need more practice in choosing the right tools for our research but do not have the research yet to do so. The game would not work as well for those who have *just* started learning about Digital History, or for those with lengthy experience. It has a very specific audience.
